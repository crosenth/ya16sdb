"""
Download and curate the NCBI 16S rRNA sequences
"""

import ConfigParser
import datetime
import errno
import os
import sys

from bioscons.slurm import SlurmEnvironment

# requirements installed in the virtualenv
from SCons.Script import (
    Variables, PathVariable, BoolVariable,
    ARGUMENTS, Help, Touch, Copy, File)

venv = os.environ.get('VIRTUAL_ENV')
if not venv:
    sys.exit('--> an active virtualenv is required'.format(venv))

true_vals = ['t', 'y', '1']
release = ARGUMENTS.get('release', 'no').lower()[0] in true_vals

vrs = Variables(None, ARGUMENTS)
vrs.Add('email', 'email address for ncbi', 'crosenth@uw.edu')
vrs.Add('retry', 'ncbi retry milliseconds', '60000')
vrs.Add('nproc', ('Number of concurrent processes '), 14)
vrs.Add('nreq', ('Number of concurrent http requests to ncbi'), 7)
vrs.Add(BoolVariable('use_cluster', 'Use slurm', False))
vrs.Add('base', help='Path to base output directory', default='output-ncbi')
vrs.Add('out',
        help='Path to dated output directory',
        default=os.path.join(
            '$base', '{:%Y%m%d}'.format(datetime.date.today())))
vrs.Add(PathVariable('training_set',
                     'path to type strain training set',
                     'data/rdp_16s_type_strains.fasta.bz2'))
vrs.Add(PathVariable('blacklist',
                     'path to custom accession blacklist',
                     'data/blacklist.txt'))

# Provides access to options prior to instantiation of env object
# below; it's better to access variables through the env object.
varargs = dict({opt.key: opt.default for opt in vrs.options}, **vrs.args)
truevals = {True, 'yes', 'y', 'True', 'true', 't'}
nproc = varargs['nproc']
use_cluster = varargs['use_cluster'] in truevals

settings = 'settings.conf'
if not os.path.exists(settings):
    print('settings.conf required for links to ncbi taxonomy '
          'use "scons settings=settings-example.conf"')
    sys.exit(1)

conf = ConfigParser.SafeConfigParser(allow_no_value=True)
conf.optionxform = str  # options are case-sensitive
conf.read(settings)

"""
Explicitly define PATH, giving preference to local executables; it's
best to use absolute paths for non-local executables rather than add
paths here to avoid accidental introduction of external
dependencies. Environment variables are inherited from the parent
shell from which scons is run and updated with the PATH defined
below, plus any additional environment variables in settings.conf
(all caps options only).
"""
environment_variables = dict(
    os.environ,
    PATH=':'.join([
        'bin',
        os.path.join(venv, 'bin'),
        '/usr/local/bin', '/usr/bin', '/bin']),
    OMP_NUM_THREADS=nproc,  # for FastTree
)

env = SlurmEnvironment(
    ENV=environment_variables,
    variables=vrs,
    use_cluster=use_cluster,
    shell='bash',
    time=False
)

# FIXME: this should be set to the default MD5-timestamp
env.Decider('MD5')

Help(vrs.GenerateHelpText(env))


def blast_db(env, sequence_file, output_base, dbtype='nucl'):
    prefix = dbtype[0]
    extensions = ['.{0}{1}'.format(prefix, suffix)
                  for suffix in ('hr', 'in', 'sq')]

    blast_out = env.Command(
        target=[output_base + ext for ext in extensions],
        source=sequence_file,
        action=('makeblastdb -dbtype {0} '
                '-in $SOURCE '
                '-out {1}'.format(dbtype, output_base)))

    env.Local(
        target=output_base,
        source=blast_out,
        action=('md5sum $SOURCES > $TARGET'))

    return blast_out


# start script
tax_latest = conf.get('DEFAULT', 'output_tax')
tax_db = conf.get('ref', 'taxdb')

"""
get accessions (versions) of records with rrna 16s annotations from ncbi

TODO: Limit to a recent date frame as initial search.  If latest records
are not found then run the whole thing in full.  This will save time in
execution of this step.

TODO: rename --threads to --server-queries (or something)
"""
ncbi = env.Command(
    source=None,
    target='$base/esearch.txt',
    action=('deenurp -v ncbi_esearch '
            '--retry $retry '
            '--threads $nreq '
            '--log $base/ncbi.log '
            '--out $TARGET '
            '$email '
            '"16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]"'))
env.AlwaysBuild(ncbi)

"""
get accessions (versions) of records considered type strains
NCBI web blast uses `sequence_from_type[Filter]` so we will use that
http://www.ncbi.nlm.nih.gov/news/01-21-2014-sequence-by-type/

NOTE: type_material[Filter] == sequence_from_type[Filter]
"""
types = env.Command(
    source=ncbi,
    target='$out/types/accessions.txt',
    action=('deenurp -v ncbi_esearch '
            '--retry $retry '
            '--threads $nreq '
            '--log $base/ncbi.log '
            '--out $TARGET '
            '$email '
            '"sequence_from_type[Filter] AND '
            '16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]"'))

"""
Touch these files in case they do not exist yet

current_accessions - text file list of the current accession
                     numbers downloaded and processed
no_features - text file list of accessions with no 16S features to be parsed
"""
current_accessions, no_features = env.Command(
    target=['$base/accessions.txt', '$base/cache/no_features.txt'],
    source=None,
    action=[Touch('${TARGETS[0]}'), Touch('${TARGETS[1]}')])
env.AlwaysBuild(current_accessions, no_features)
env.Precious(current_accessions, no_features)

"""
A grep diff between sequences we have, the blacklist and what
ncbi returns.  This will give us accession (versions) that are
not present to pass to entrez.py for download.
"""
new_accessions = env.Command(
    target=['$out/new_accessions.txt'],
    source=[current_accessions, no_features, ncbi],
    action=[Touch('$TARGET'),
            'cat ${SOURCES[:2]} $blacklist | '
            'grep '
            '--invert-match '
            '--fixed-strings '
            '--file /dev/stdin '
            '${SOURCES[2]} > $TARGET '
            # avoid the grep no match exit code 1 that scons hates
            '|| true'])

"""
Download gb files and write fasta and seq_info right away
"""
raw_fasta, raw_info, raw_pubmed_ids, raw_references = env.Command(
    source=[no_features, types, new_accessions],
    target=['$base/cache/raw.fasta',
            '$base/cache/raw.csv',
            '$base/cache/raw_pubmed_ids.csv',
            '$base/cache/raw_references.csv'],
    action=('deenurp -v ncbi_extract_genbank '
            '--strand 1 '
            '--chunksize 1 '
            '--retry $retry '
            '--feature rrna::16s '
            '--log $base/ncbi.log '
            '--threads $nreq '
            '--no-features ${SOURCES[0]} '
            '--type-strains ${SOURCES[1]} '
            '--pubmed_ids ${TARGETS[2]} '
            '--references ${TARGETS[3]} '
            '$email ${SOURCES[2]} ${TARGETS[:2]}'))
env.Precious(raw_fasta, raw_info, raw_pubmed_ids, raw_references)

"""
Because records are appended and ncbi errors are common we need to
check sequences here for inconsistencies between
raw.fasta, raw.csv, raw_pubmed_ids and raw_references.csv
"""
sync_fasta, sync_info, sync_pubmed_ids, sync_references = env.Command(
    source=[raw_fasta,
            raw_info,
            raw_pubmed_ids,
            raw_references,
            no_features,
            current_accessions],
    target=['$base/cache/sync.fasta',
            '$base/cache/sync.csv',
            '$base/cache/sync_pubmed_ids.csv',
            '$base/cache/sync_references.csv'],
    action=['sync_records.py '
            '--accessions ${SOURCES[5]} '
            '${SOURCES[:5]} $TARGETS',
            # overwrite the "raw" files with the sync files
            Copy('${SOURCES[0]}', '${TARGETS[0]}'),
            Copy('${SOURCES[1]}', '${TARGETS[1]}'),
            Copy('${SOURCES[2]}', '${TARGETS[2]}'),
            Copy('${SOURCES[3]}', '${TARGETS[3]}')])

"""
Check sequence orientation and 16s region

TODO: save state of outputs to avoid extra work on records already processed
TODO: analyze no_16s matches, we want to make sure
      we're not losing valid sequences.
TODO: need pubs output here

FIXME: add a filter by coverage

TODO: break this up into vsearch and a script
      that filters fixes the orientation
"""
orientated, orientated_info, _, _, _ = env.Command(
    target=['$base/orientated/seqs.fasta',
            '$base/orientated/seq_info.csv',
            '$base/orientated/alignments.csv',
            '$base/orientated/notmatched.fasta',
            '$base/orientated/notmatched.csv'],
    source=[sync_fasta, '$training_set', sync_info],
    action=('deenurp orientate_sequences '
            '--threads $nproc '
            '--id 0.70 '
            '--log $base/orientated/vsearch.log '
            '--seq_info ${SOURCES[2]} '
            '--out ${TARGETS[0]} '
            '--out_seq_info ${TARGETS[1]} '
            '--out_csv ${TARGETS[2]} '
            '--out_notmatched ${TARGETS[3]} '
            '--out_notmatched_info ${TARGETS[4]} '
            '${SOURCES[:2]}'))

"""
Copy over files to release dir for processing
"""
seqs, pubmed_ids, references = env.Command(
    target=['$out/seqs.fasta',
            '$out/pubmed_ids.csv',
            '$out/references.csv'],
    source=[orientated, sync_pubmed_ids, sync_references],
    action=[Copy('${TARGETS[0]}', '${SOURCES[0]}'),
            Copy('${TARGETS[1]}', '${SOURCES[1]}'),
            Copy('${TARGETS[2]}', '${SOURCES[2]}')])

"""
update seq_info with latest tax_ids and add taxid_classified column
"""
seq_info, _ = env.Command(
    target=['$out/seq_info.csv', '$out/unknown_taxons.csv'],
    source=[orientated_info, tax_db],
    action=['taxit -v update_taxids '
            '--name-column organism '
            '--taxid-classified '
            '--unknowns ${TARGETS[1]} '
            '--out ${TARGETS[0]} '
            '$SOURCES'])

"""
Make general taxtable
"""
tax = env.Command(
    target='$out/taxonomy.csv',
    source=[tax_db,
            File('taxonomy.csv', tax_latest),
            seq_info],
    action=('taxit -v taxtable '
            '--from-table ${SOURCES[1]} '
            '--seq-info ${SOURCES[2]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
tax_counts = env.Command(
    target='$out/tax_counts.csv',
    source=[tax, seq_info],
    action=('taxit -v count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Deduplicate sequences by isolate (accession)
"""
dedup_fasta, dedup_info = env.Command(
    target=['$out/dedup.fasta',
            '$out/dedup.csv'],
    source=[seqs, seq_info],
    action=('deenurp -v deduplicate_sequences '
            '--group-by accession '
            '--prefer-columns is_type '
            '$SOURCES $TARGETS'))

"""
Drop sequences < 1200 bp and with more than 1% ambig and
partition into named and unnamed data sets.
"""
named_fasta, named_seqinfo, unnamed_fasta, unnamed_seqinfo = env.Command(
    target=['$out/named/seqs.fasta',
            '$out/named/seq_info.csv',
            '$out/unnamed/seqs.fasta',
            '$out/unnamed/seq_info.csv'],
    source=[dedup_fasta, dedup_info],
    action=('deenurp -v partition_refs '
            # filtering
            '--min-length 1200 '
            '--prop-ambig-cutoff 0.01 '
            '--named-seqs ${TARGETS[0]} --named-info ${TARGETS[1]} '
            '--unnamed-seqs ${TARGETS[2]} --unnamed-info ${TARGETS[3]} '
            '$SOURCES'))

"""
Make named taxtable
"""
named_tax = env.Command(
    target='$out/named/taxonomy.csv',
    source=[tax_db, named_seqinfo, tax],
    action=('taxit -v taxtable '
            '--full '
            '--from-table ${SOURCES[2]} '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
named_tax_counts = env.Command(
    target='$out/named/tax_counts.csv',
    source=[named_tax, named_seqinfo],
    action=('taxit -v count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Make unnamed taxtable
"""
unnamed_tax = env.Command(
    target='$out/unnamed/taxonomy.csv',
    source=[tax_db, unnamed_seqinfo, tax],
    action=('taxit -v taxtable '
            '--full '
            '--from-table ${SOURCES[2]} '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
unnamed_tax_counts = env.Command(
    target='$out/unnamed/tax_counts.csv',
    source=[unnamed_tax, unnamed_seqinfo],
    action=('taxit -v count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Create blast databases for all, named and unnamed
"""
all_blast = blast_db(env, seqs, '$out/blast')
named1200_blast = blast_db(env, named_fasta, '$out/named/blast')
unnamed1200_blast = blast_db(env, unnamed_fasta, '$out/unnamed/blast')

"""
Partition type strains
"""
types_fasta, types_seqinfo = env.Command(
    target=['$out/types/seqs.fasta',
            '$out/types/seq_info.csv'],
    source=[named_fasta, named_seqinfo],
    action=('deenurp -v partition_refs '
            '--type-seqs ${TARGETS[0]} '
            '--type-info ${TARGETS[1]} '
            '$SOURCES'))

"""
Make type strain taxtable
"""
types_tax = env.Command(
    target='$out/types/taxonomy.csv',
    source=[tax_db, types_seqinfo, tax],
    action=('taxit -v taxtable '
            '--full '
            '--from-table ${SOURCES[2]} '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
unnamed_ref_counts = env.Command(
    target='$out/types/tax_counts.csv',
    source=[types_tax, types_seqinfo],
    action=('taxit -v count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Make type strain Blast database
"""
types_blast = blast_db(env, types_fasta, '$out/types/blast')

"""
dereplicate sequences using vsearch clustering to reduce giant species
clusters like Bacilius genus and E. Coli
"""
derep_fasta, derep_seqinfo, derep_map = env.Command(
    target=['$out/filtered/derep.fasta',
            '$out/filtered/derep_info.csv',
            '$out/filtered/derep_map.csv'],
    source=[named_fasta, named_seqinfo, named_tax],
    action=('deenurp -v dereplicate_named '
            '--id 1.0 '
            '--taxonomy ${SOURCES[2]} '
            '--seqs-out ${TARGETS[0]} '
            '--seq-info-out ${TARGETS[1]} '
            '--derep-map-out ${TARGETS[2]} '
            '${SOURCES[:2]}'))

"""
Filter sequences. Use --threads if you need to to limit the number
of processes - otherwise deenurp will use all of them!

FIXME: consider first running taxit update_taxids on previous-details
FIXME: create a new taxtable for derep_fasta and derep_info
"""
filtered_fasta, filtered_seqinfo, filtered_details = env.Command(
    source=[derep_fasta, derep_seqinfo, named_tax],
    target=['$out/filtered/seqs.fasta',
            '$out/filtered/seq_info.csv',
            '$out/filtered/details.csv'],
    action=['deenurp -vvv filter_outliers '
            '--log $base/deenurp.log '
            '--filter-rank species '
            '--threads-per-job 14 '
            '--jobs 1 '
            '--output-seqs ${TARGETS[0]}  '
            '--filtered-seqinfo ${TARGETS[1]} '
            '--detailed-seqinfo ${TARGETS[2]} '
            '--previous-details $base/cache/details.csv '
            '--strategy cluster '
            '--cluster-type single '
            '--distance-percentile 90.0 '
            '--min-distance 0.01 '
            '--max-distance 0.02 '
            '--min-seqs-for-filtering 5 '
            '$SOURCES',
            Copy('$base/cache/details.csv', '${TARGETS[2]}')],
    slurm_queue='full')

# find top hit for each sequence among type strains
type_hits = env.Command(
    target='$out/filtered/vsearch.blast6out',
    source=[derep_fasta, types_fasta],
    action=('vsearch --usearch_global ${SOURCES[0]} --db ${SOURCES[1]} '
            '--blast6out $TARGET --id 0.75 --threads 12 '
            '--maxaccepts 1 --strand plus'))

'''
bokeh plot filtered sequences

hard coded: sort column 2 (records) desc

FIXME: BokehDeprecationWarning 2> /dev/null
'''
env.Command(
    target=['$out/filtered/index.html', '$out/filtered/plots/index.html'],
    source=[filtered_details, named_tax, type_hits, types_seqinfo],
    action=('plot_details.py $SOURCES '
            '--param strategy:cluster '
            '--param cluster_type:single '
            '--param distance_percentile:90.0 '
            '--param min_distance:0.01 '
            '--param max_distance:0.02 '
            '--log $base/deenurp.log '
            '--plot-dir $out/filtered/plots '
            '--plot-map ${TARGETS[1]} '
            '--plot-index ${TARGETS[0]} '
            '2> /dev/null'
            ))

"""
Make filtered taxtable
"""
filtered_tax = env.Command(
    target='$out/filtered/taxonomy.csv',
    source=[tax_db, filtered_seqinfo, named_tax],
    action=('taxit -v taxtable '
            '--full '
            '--from-table ${SOURCES[2]} '
            '--seq-info ${SOURCES[1]} '
            '--out-file $TARGET '
            '${SOURCES[0]}'))

filtered_blast = blast_db(env, filtered_fasta, '$out/filtered/blast')

"""
Count reference sequences that can be used for classification filtering
"""
filtered_ref_counts = env.Command(
    target='$out/filtered/tax_counts.csv',
    source=[filtered_tax, filtered_seqinfo],
    action=('taxit -v count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Append contributers
"""
contributors = env.Command(
    source='.git/logs/HEAD',
    target='contributors.txt',
    action=('git log --all --format="%cN <%cE>" | sort | uniq > $TARGET'))

"""
release steps
"""
if release:
    def SymLink(directory='.'):
        '''
        scons does not manage files outside its base directory so we work
        around that by passing the symlink directory as a separate argument not
        managed by scons
        '''
        def SymLinkAction(target, source, env):
            src = os.path.abspath(str(source[0]))
            targ = os.path.abspath(os.path.join(directory, str(target[0])))
            try:
                os.symlink(src, targ)
            except OSError, e:
                if e.errno == errno.EEXIST:
                    os.remove(targ)
                    os.symlink(src, targ)
                else:
                    raise e

        return SymLinkAction

    '''
    create symbolic link LATEST on directory up to point to $out dir
    '''
    latest = env.Command(
        target=os.path.join('$base', 'LATEST'),
        source='$out',
        action=SymLink())

    commit = env.Command(
        target='$out/version.txt',
        source='.git/objects',
        action='git describe --tags > $TARGET')

    freeze = env.Command(
        target='$out/requirements.txt',
        source=venv,
        action='pip freeze > $TARGET')
