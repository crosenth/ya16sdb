"""
Download and curate the NCBI 16S rRNA sequences
"""

import datetime
import errno
import os
import sys

from bioscons.slurm import SlurmEnvironment

# requirements installed in the virtualenv
from SCons.Script import (
    Variables, PathVariable, BoolVariable, ARGUMENTS, Help, Copy)

venv = os.environ.get('VIRTUAL_ENV')
if not venv:
    sys.exit('--> an active virtualenv is required'.format(venv))

true_vals = ['t', 'y', '1']
release = ARGUMENTS.get('release', 'no').lower()[0] in true_vals

vrs = Variables(None, ARGUMENTS)
vrs.Add('email', 'email address for ncbi', 'crosenth@uw.edu')
vrs.Add('retry', 'ncbi retry milliseconds', '60000')
vrs.Add('nproc', ('Number of concurrent processes '), 14)
vrs.Add('nreq', ('Number of concurrent http requests to ncbi'), 3)
vrs.Add(BoolVariable('use_cluster', 'Use slurm', False))
vrs.Add('base', help='Path to base output directory', default='output-ncbi')
vrs.Add(
    'tax_url',
    default='"postgresql://crosenth:password@db3.labmed.uw.edu/molmicro"',
    help='database url')
vrs.Add(
    'out',
    help='Path to dated output directory',
    default=os.path.join('$base', '{:%Y%m%d}'.format(datetime.date.today())))
vrs.Add(PathVariable('training_set',
                     'path to type strain training set',
                     'data/rdp_16s_type_strains.fasta.bz2'))
vrs.Add(PathVariable('blacklist',
                     'path to custom accession blacklist',
                     'data/blacklist.txt'))

# cache
vrs.Add('seqs_cache', default='$base/seqs.fasta')
vrs.Add('seq_info_cache', default='$base/seq_info.csv')
vrs.Add('pubmed_ids_cache', default='$base/pubmed_ids.csv')
vrs.Add('references_cache', default='$base/references.csv')
vrs.Add('versions_cache', default='$base/versions.txt')

# Provides access to options prior to instantiation of env object
# below; it's better to access variables through the env object.
varargs = dict({opt.key: opt.default for opt in vrs.options}, **vrs.args)
truevals = {True, 'yes', 'y', 'True', 'true', 't'}
nproc = varargs['nproc']
use_cluster = varargs['use_cluster'] in truevals

"""
Explicitly define PATH, giving preference to local executables; it's
best to use absolute paths for non-local executables rather than add
paths here to avoid accidental introduction of external
dependencies. Environment variables are inherited from the parent
shell from which scons is run and updated with the PATH defined
below, plus any additional environment variables in settings.conf
(all caps options only).
"""
environment_variables = dict(
    os.environ,
    PATH=':'.join([
        'bin',
        os.path.join(venv, 'bin'),
        '/usr/local/bin', '/usr/bin', '/bin']),
    OMP_NUM_THREADS=nproc,  # for FastTree
)

env = SlurmEnvironment(
    ENV=environment_variables,
    variables=vrs,
    use_cluster=use_cluster,
    shell='bash',
    time=False
)

env.Decider('MD5')

Help(vrs.GenerateHelpText(env))


def blast_db(env, sequence_file, output_base, dbtype='nucl'):
    '''
    Create a blast database
    '''
    prefix = dbtype[0]
    extensions = ['.{0}{1}'.format(prefix, suffix)
                  for suffix in ('hr', 'in', 'sq')]

    blast_out = env.Command(
        target=[output_base + ext for ext in extensions],
        source=sequence_file,
        action=('makeblastdb -dbtype {0} '
                '-in $SOURCE '
                '-out {1}'.format(dbtype, output_base)))

    env.Local(
        target=output_base,
        source=blast_out,
        action=('md5sum $SOURCES > $TARGET'))

    return blast_out


"""
get accessions (versions) of records with rrna 16s annotations from ncbi

TODO: Limit to a recent date frame as initial search.  If latest records
are not found then run the whole thing in full.  This will save time in
execution of this step.
"""
ncbi = env.Command(
    source=None,
    target='$out/ncbi.txt',
    action=('esearch '
            '-db nucleotide '
            '-query "16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]" | '
            'mefetch '
            '-email $email '
            '-mode text '
            '-format acc '
            '-retry $retry '
            '-proc $nreq '
            '-log $out/ncbi.log '
            '-out $TARGET'))

"""
A grep diff between sequences we have, the blacklist and what
ncbi returns.  This will give us accession (versions) that are
not present to pass to entrez.py for download.
"""
versions = env.Command(
    target='$out/new/versions.txt',
    source=['data/blacklist.txt', ncbi],
    action=['cat $versions_cache ${SOURCES[0]} | '
            'grep '
            '--invert-match '
            '--fixed-strings '
            '--file /dev/stdin '
            '${SOURCES[1]} > $TARGET '
            # avoid the grep no match exit code 1 that scons hates
            '|| true'])

"""
Download records

1) input as accessions
2) mefetch feature tables
3) ftract 16s rrna features
4) mefetch gb records
5) write fasta, seq_info, references and pubmed_ids
"""
new_fasta, new_info, new_pubmed_ids, new_references = env.Command(
    target=['$out/new/seqs.fasta',
            '$out/new/seq_info.csv',
            '$out/new/pubmed_ids.csv',
            '$out/new/references.csv'],
    source=versions,
    action=('mefetch '  # download feature tables
            '-email $email '
            '-retry $retry '
            '-id $SOURCE '
            '-db nucleotide '
            '-format ft '
            '-retmax 1 '
            '-log $out/ncbi.log '
            '-proc $nreq | '
            'ftract -feature rrna::16s | '   # extract 16s features
            'mefetch '  # download genbank records
            '-email $email '
            '-retmax 1 '
            '-csv '
            '-db nucleotide '
            '-format gbwithparts '
            '-log $out/ncbi.log '
            '-proc $nreq '
            '-retry $retry | '
            'extract_genbank.py '  # extract genbank info into csv
            '--pubmed_ids ${TARGETS[2]} '
            '--references ${TARGETS[3]} '
            '${SOURCES[2]} ${TARGETS[:2]}'))

"""
Record versions returned from esearch that had no actual 16s features
"""
no_features = env.Command(
    target='$out/new/no_features.txt',
    source=[new_info, versions],
    action=('csvcut.py --columns version ${SOURCES[0]} | '
            'tail -n +2 | '
            'grep '
            '--invert-match '
            '--fixed-strings '
            '--file /dev/stdin '
            '${SOURCES[1]} > $TARGET '
            # avoid the grep no match exit code 1 that scons hates
            '|| true'))

"""
filter invalid tax_ids

Do nothing with the unknown records for now because it might simply mean
the ncbi taxonomy pipeline is out of sync with the latest 16s records
"""
known_info, _ = env.Command(
    target=['$out/new/valid/seq_info.csv', '$out/new/invalid/seq_info.csv'],
    source=new_info,
    action=['taxit update_taxids '
            '--name-column organism '
            '--unknowns ${TARGETS[1]} '
            '--out ${TARGETS[0]} '
            '--schema ncbi '
            '$SOURCES $tax_url'])

"""
vsearch new sequences with training set to test sequence orientation
and 16s region
"""
vsearch = env.Command(
    target='$out/new/vsearch.csv',
    source=[new_fasta, 'data/rdp_16s_type_strains.fasta.bz2'],
    action=('vsearch '
            '--usearch_global ${SOURCES[0]} '
            '--db ${SOURCES[1]} '
            '--id 0.70 '
            '--threads 14 '
            '--userfields query+target+qstrand+id+tilo+tihi '
            '--strand both '
            '--top_hits_only '
            '--output_no_hits '
            '--maxaccepts 1 '  # default is 1
            '--maxrejects 32 '  # default is 32
            '--userout $TARGET'))

"""
1) Fix sequence orientation of new sequences
2) Remove sequences not in 16s region
3) MOVED: Add is_type column
4) Deduplicate pubmed_ids and references from
   multiple 16s regions per accession
5) Append seqs, seq_info, pubmed_ids and references to previous data set
7) drop seqnames missing eigher a sequence or row in seq_info, sequences
   filtered out of the vsearch 16s alignment or sequences with unknown tax_ids
8) Copy full dataset back to base dir for next download

NOTE: seqs that failed either the vsearch or taxit update_taxids will
not be appended to the versions.txt file and will therefore be re-downloaded
the next time this pipeline is run
"""
all_fa, appended_info, pubmed_ids, references, _ = env.Command(
    target=['$out/seqs.fasta',
            '$out/appended_info.csv',
            '$out/pubmed_ids.csv',
            '$out/references.csv',
            '$out/versions.txt'],
    source=[new_fasta, '$seqs_cache',
            known_info, '$seq_info_cache',
            new_pubmed_ids, '$pubmed_ids_cache',
            new_references, '$references_cache',
            no_features, '$versions_cache',
            vsearch],
    action=['annotate_and_append.py $SOURCES $TARGETS',
            # cache for next download
            Copy('$seqs_cache', '${TARGETS[0]}'),
            Copy('$pubmed_ids_cache', '${TARGETS[2]}'),
            Copy('$references_cache', '${TARGETS[3]}'),
            Copy('$versions_cache', '${TARGETS[4]}')])

"""
get accessions (versions) of records considered type strains
NCBI web blast uses `sequence_from_type[Filter]` so we will use that
http://www.ncbi.nlm.nih.gov/news/01-21-2014-sequence-by-type/

NOTE: type_material[Filter] == sequence_from_type[Filter]
"""
types = env.Command(
    source=ncbi,
    target='$out/types.txt',
    action=('esearch '
            '-db nucleotide '
            '-query "sequence_from_type[Filter] AND '
            '16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]" | '
            'mefetch '
            '-email $email '
            '-mode text '
            '-format acc '
            '-proc $nreq '
            '-retry $retry '
            '-log $out/ncbi.log '
            '-out $TARGET'))

"""
Remove and re-append is_type column with sequences in:

1) types set above
2) sequences with 'ATCC' in description
3) sequences with 'NR_' to start version number
"""
types_appended_info = env.Command(
    target='$out/is_type_info.csv',
    source=[appended_info, types],
    action='is_type.py $SOURCES $TARGET')

"""
update tax_ids and append taxid_classified column
"""
update_all_info = env.Command(
    target='$out/seq_info.csv',
    source=types_appended_info,
    action=['taxit -v update_taxids '
            '--name-column organism '
            '--out $TARGET '
            '--schema ncbi '
            '$SOURCE $tax_url',
            Copy('$seq_info_cache', '$TARGET')])

"""
pull sequences at least 1200 bp and less than 1% ambiguous
"""
fa, seq_info = env.Command(
    target=['$out/1200bp/seqs.fasta', '$out/1200bp/seq_info.csv'],
    source=[all_fa, update_all_info],
    action=('partition_refs.py '
            # filtering
            '--min-length 1200 '
            '--prop-ambig-cutoff 0.01 '
            '--out-fa ${TARGETS[0]} '
            '--out-info ${TARGETS[1]} '
            '$SOURCES'))

"""
Make general classified taxtable
"""
tax = env.Command(
    target='$out/1200bp/classified/taxonomy.csv',
    source=seq_info,
    action=('taxit -v taxtable '
            '--seq-info $SOURCE '
            '--clade-ids 2 '  # Bacteria
            '--valid '
            '--ranked '
            '--out $TARGET '
            '--schema ncbi '
            '$tax_url'))

"""
pull classified sequences based on valid and ranked tax_ids
"""
classified_fa, classified_info = env.Command(
    target=['$out/1200bp/classified/seqs.fasta',
            '$out/1200bp/classified/seq_info.csv'],
    source=[tax, fa, seq_info],
    action=('partition_refs.py '
            '--tax-ids ${SOURCES[0]} '
            '--out-fa ${TARGETS[0]} '
            '--out-info ${TARGETS[1]} '
            '${SOURCES[1:]}'))

"""
Count reference sequences that can be used for classification filtering
"""
env.Command(
    target='$out/1200bp/classified/tax_counts.csv',
    source=tax,
    action='taxit -v count_taxids --out $TARGET $SOURCE')

"""
Create blast database classified seqs
"""
blast_db(env, classified_fa, '$out/1200bp/classified/blast')

"""
Partition type strains
"""
types_fasta, types_info = env.Command(
    target=['$out/1200bp/classified/types/seqs.fasta',
            '$out/1200bp/classified/types/seq_info.csv'],
    source=[classified_fa, classified_info],
    action=('partition_refs.py '
            '--types '
            '--out-fa ${TARGETS[0]} '
            '--out-info ${TARGETS[1]} '
            '$SOURCES'))

"""
Make type strain taxtable
"""
types_tax = env.Command(
    target='$out/1200bp/classified/types/taxonomy.csv',
    source=[tax, types_info],
    action=('taxit -v taxtable '
            '--taxtable ${SOURCES[0]} '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '--schema ncbi '
            '$tax_url'))

"""
Count reference sequences that can be used for classification filtering
"""
env.Command(
    target='$out/1200bp/classified/types/tax_counts.csv',
    source=types_tax,
    action='taxit -v count_taxids --out $TARGET $SOURCE')

"""
Make type strain Blast database
"""
blast_db(env, types_fasta, '$out/1200bp/classified/types/blast')

"""
Deduplicate sequences by isolate (accession)

Appends a weight column showing number of sequences being represented
"""
dedup_fa, dedup_info = env.Command(
    target=['$out/1200bp/classified/dedup/seqs.fasta',
            '$out/1200bp/classified/dedup/seq_info.csv'],
    source=[classified_fa, classified_info],
    action=('deenurp -v deduplicate_sequences '
            '--group-by accession '
            '--prefer-columns is_type '
            '$SOURCES $TARGETS'))

"""
dereplicate sequences using vsearch clustering to reduce giant species
clusters like Bacilius genus and E. Coli

NOTE: we might not need to do this anymore now that filter_outliers has enough
resources to finish in a reasonable amount of time
"""
# derep_fa, derep_info = env.Command(
#     target=['$out/1200bp/classified/dedup/derep/seqs.fasta',
#             '$out/1200bp/classified/dedup/derep/seq_info.csv',
#             '$out/1200bp/classified/dedup/derep/map.csv'],
#     source=[dedup_fa, dedup_info, tax],
#     action=('deenurp -v dereplicate_named '
#             '--id 1.0 '
#             '--group-on species '
#             '--taxonomy ${SOURCES[2]} '
#             '--seqs-out ${TARGETS[0]} '
#             '--seq-info-out ${TARGETS[1]} '
#             '--derep-map-out ${TARGETS[2]} '
#             '${SOURCES[:2]}'))

filtered_details_in = env.Command(
    source='$base/filtered_details.csv',
    target='$out/1200bp/classified/filtered/details_in.csv',
    action='taxit update_taxids --schema ncbi --out $TARGET $SOURCE $tax_url')

"""
Filter sequences. Use --threads if you need to to limit the number
of processes - otherwise deenurp will use all of them!
"""
filtered_fa, filtered_info, filtered_details, deenurp_log = env.Command(
    source=[dedup_fa, dedup_info, tax, filtered_details_in],
    target=['$out/1200bp/classified/filtered/seqs.fasta',
            '$out/1200bp/classified/filtered/seq_info.csv',
            '$out/1200bp/classified/filtered/details_out.csv',
            '$out/1200bp/classified/filtered/deenurp.log'],
    action=['deenurp -vvv filter_outliers '
            '--log ${TARGETS[3]} '
            '--filter-rank species '
            '--threads-per-job 14 '
            '--jobs 1 '
            '--output-seqs ${TARGETS[0]}  '
            '--filtered-seqinfo ${TARGETS[1]} '
            '--detailed-seqinfo ${TARGETS[2]} '
            '--previous-details ${SOURCES[3]} '
            '--strategy cluster '
            '--cluster-type single '
            '--distance-percentile 90.0 '
            '--min-distance 0.01 '
            '--max-distance 0.02 '
            '--min-seqs-for-filtering 10 '
            '${SOURCES[:3]}',
            # cache this
            Copy('$base/filtered_details.csv', '${TARGETS[2]}')])

"""
Make filtered taxtable
"""
filtered_tax = env.Command(
    target='$out/1200bp/classified/filtered/taxonomy.csv',
    source=[tax, filtered_info],
    action=('taxit -v taxtable '
            '--taxtable ${SOURCES[0]} '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '--schema ncbi '
            '$tax_url'))

'''
find top hit for each sequence among type strains

take --maxaccepts 2 so we can filter out hits where the query and target
sequences are the same
'''
type_hits = env.Command(
    target='$out/1200bp/classified/filtered/vsearch.blast6out',
    source=[dedup_fa, types_fasta],
    action=('vsearch --usearch_global ${SOURCES[0]} '
            '--db ${SOURCES[1]} '
            '--blast6out $TARGET '
            '--id 0.75 '
            '--threads 14 '
            '--self '  # reject same sequence hits
            '--threads 12 '
            '--maxaccepts 1 '
            '--strand plus'))

'''
bokeh plot filtered sequences

hard coded: sort column 2 (records) desc

FIXME: BokehDeprecationWarning 2> /dev/null

TODO: include derep_map.csv to input for
"dereplicated" sequence counts per accession
'''
env.Command(
    target=['$out/1200bp/classified/filtered/index.html',
            '$out/1200bp/classified/filtered/plots/map.csv'],
    source=[filtered_details, tax, type_hits, types_info, deenurp_log],
    action=('plot_details.py ${SOURCES[:4]} '
            '--param strategy:cluster '
            '--param cluster_type:single '
            '--param distance_percentile:90.0 '
            '--param min_distance:0.01 '
            '--param max_distance:0.02 '
            '--log-in ${SOURCES[4]} '
            '--plot-dir $out/1200bp/classified/filtered/plots '
            '--plot-map ${TARGETS[1]} '
            '--plot-index ${TARGETS[0]}'
            ))

blast_db(env, filtered_fa, '$out/1200bp/classified/filtered/blast')

"""
Count reference sequences that can be used for classification filtering
"""
env.Command(
    target='$out/1200bp/classified/filtered/tax_counts.csv',
    source=filtered_tax,
    action='taxit -v count_taxids --out $TARGET $SOURCE')

"""
Append contributers
"""
contributors = env.Command(
    source='.git/logs/HEAD',
    target='contributors.txt',
    action='git log --all --format="%cN <%cE>" | sort | uniq > $TARGET')

"""
release steps
"""
if release:
    def SymLink(directory='.'):
        '''
        scons does not manage files outside its base directory so we work
        around that by passing the symlink directory as a separate argument not
        managed by scons
        '''
        def SymLinkAction(target, source, env):
            src = os.path.abspath(str(source[0]))
            targ = os.path.abspath(os.path.join(directory, str(target[0])))
            try:
                os.symlink(src, targ)
            except OSError as e:
                if e.errno == errno.EEXIST:
                    os.remove(targ)
                    os.symlink(src, targ)
                else:
                    raise e

        return SymLinkAction

    '''
    create symbolic link LATEST on directory up to point to $out dir
    '''
    latest = env.Command(
        target=os.path.join('$base', 'LATEST'),
        source='$out',
        action=SymLink())

    commit = env.Command(
        target='$out/git_version.txt',
        source='.git/objects',
        action='git describe --tags > $TARGET')

    freeze = env.Command(
        target='$out/requirements.txt',
        source=venv,
        action='pip freeze > $TARGET')
