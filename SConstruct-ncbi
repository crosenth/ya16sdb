"""
Download and curate the NCBI 16S rRNA sequences
"""

import os
import sys

venv = os.environ.get('VIRTUAL_ENV')
if not venv:
    sys.exit('--> an active virtualenv is required'.format(venv))

# requirements installed in the virtualenv
from SCons.Script import (
    Variables, PathVariable, BoolVariable,
    ARGUMENTS, Help, Touch, Flatten, Copy)

from bioscons.slurm import SlurmEnvironment

version = 'release'

vars = Variables(None, ARGUMENTS)
vars.Add(PathVariable('out', 'Path to output directory', 'output-ncbi',
         PathVariable.PathIsDirCreate))
vars.Add(PathVariable('data', 'Path to data directory', 'data',
         PathVariable.PathIsDirCreate))
vars.Add('nproc', ('Number of concurrent processes '), 25)
vars.Add('nreq', ('Number of concurrent http requests to ncbi'), 3)
vars.Add(BoolVariable('use_cluster', 'Use slurm', False))
# FIXME: this will error if it doesn't exist
vars.Add(PathVariable('blacklist',
                      'path to blastlist.txt',
                      '$data/blacklist.txt'))
# FIXME: this will error if it doesn't exist
vars.Add(PathVariable('have_records',
                      'path to have_records.txt',
                      '$out/have_records.txt'))
vars.Add(PathVariable('training_set',
                      'path to type strain training set',
                      '$data/rdp_16s_type_strains.fasta.bz2'))
# FIXME: this will error if it doesn't exist
vars.Add(PathVariable('log',
                      'path to type strain training set',
                      '$out/ncbi.log'))
vars.Add('email', 'email address for ncbi', 'crosenth@uw.edu')
vars.Add('retry', 'ncbi retry milliseconds', '60000')

# Provides access to options prior to instantiation of env object
# below; it's better to access variables through the env object.
varargs = dict({opt.key: opt.default for opt in vars.options}, **vars.args)
truevals = {True, 'yes', 'y', 'True', 'true', 't'}
nproc = varargs['nproc']
use_cluster = varargs['use_cluster'] in truevals

"""
Explicitly define PATH, giving preference to local executables; it's
best to use absolute paths for non-local executables rather than add
paths here to avoid accidental introduction of external
dependencies. Environment variables are inherited from the parent
shell from which scons is run and updated with the PATH defined
below, plus any additional environment variables in settings.conf
(all caps options only).
"""
environment_variables = dict(
    os.environ,
    PATH=':'.join([
        'bin',
        os.path.join(venv, 'bin'),
        '/usr/local/bin', '/usr/bin', '/bin']),
    OMP_NUM_THREADS=nproc,  # for FastTree
)

base_env = SlurmEnvironment(
    ENV=environment_variables,
    variables=vars,
    use_cluster=use_cluster,
    shell='bash',
    version=version,
    time=False
)

base_env.Decider('MD5')

Help(vars.GenerateHelpText(base_env))


def blast_db(env, sequence_file, output_base, dbtype='nucl'):
    prefix = dbtype[0]
    extensions = ['.{0}{1}'.format(prefix, suffix)
                  for suffix in ('hr', 'in', 'sq')]

    blast_out = env.Command(
        target=[output_base + ext for ext in extensions],
        source=sequence_file,
        action=('makeblastdb -dbtype {0} '
                '-in $SOURCE '
                '-out {1}'.format(dbtype, output_base)))

    env.Local(
        target=output_base,
        source=blast_out,
        action=('md5sum $SOURCES > $TARGET'))

    return blast_out


def combine_blast_dbs(env, db_files, output_base, title, dbtype='nucl'):
    prefix = dbtype[0]
    extensions = ['.{0}{1}'.format(prefix, suffix) for suffix in ['al']]
    source_dbs = sorted({os.path.abspath(os.path.splitext(str(i))[0])
                         for i in db_files})

    return env.Local(
        target=[output_base + ext for ext in extensions],
        source=db_files,
        action=('blastdb_aliastool -out {base} -dbtype {dbtype} '
                '-dblist "{dblist}" -title "{title}"').format(
            base=output_base,
            dbtype=dbtype,
            dblist=' '.join(source_dbs),
            title=title)
    )

# start script

"""
get accessions (versions) of records with rrna 16s annotations from ncbi
"""
ncbi = base_env.Command(
    source=None,
    target='$out/all_records.txt',
    action=('deenurp -v ncbi_esearch '
            '--retry $retry '
            '--threads $nreq '
            '--log $log '
            '--out $TARGET '
            '$email '
            '"16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]"'))
# base_env.AlwaysBuild(ncbi)

"""
get accessions (versions) of records considered type strains

NCBI web blast uses `sequence_from_type[Filter]` so we will use that
NOTE: type_material[Filter] == sequence_from_type[Filter]
"""
types = base_env.Command(
    source=ncbi,
    target='$out/type_records.txt',
    action=('deenurp -v ncbi_esearch '
            '--retry $retry '
            '--threads $nreq '
            '--log $log '
            '--out $TARGET '
            '$email '
            '"sequence_from_type[Filter] AND '
            '16s[All Fields] AND '
            'rRNA[Feature Key] AND '
            'Bacteria[Organism] '
            'NOT(environmental samples[Organism] '
            'OR metagenomes[Organism] '
            'OR txid32644[Organism]) '
            'AND 500 : 999999999[Sequence Length]"'))

"""
A grep diff between sequences we have, the blacklist and what
ncbi returns.  This will give us accession (versions) that are
not present to pass to entrez.py for download.
"""
accessions = base_env.Command(
    target='$out/no_records.txt',
    source=ncbi,
    action=[Touch('$have_records'), Touch('$blacklist'),
            'cat $have_records $blacklist | '
            'grep '
            '--invert-match '
            '--fixed-strings '
            '--file /dev/stdin '
            '$SOURCE > $TARGET'])

"""
Download gb files and write fasta and seq_info right away
"""
raw_fasta, raw_info, raw_pubs = base_env.Command(
    source=[accessions, types],
    target=['$out/raw.fasta', '$out/raw.csv', '$out/raw_pubs.csv'],
    action=('deenurp -v ncbi_extract_genbank '
            '--no-features $blacklist '
            '--strand 1 '
            '--chunksize 1 '
            '--retry $retry '
            '--type-strains ${SOURCES[1]} '
            '--feature rrna::16s '
            '--log $log '
            '--threads $nreq '
            '$email ${SOURCES[0]} $TARGETS'))
base_env.Precious(raw_fasta, raw_info, raw_pubs)

"""
Because records are appended and ncbi errors are common we need to
check sequences here for inconsistencies between
records.fasta, records.csv and references.csv
"""
sync_fasta, sync_info, sync_pubs = base_env.Command(
    source=[raw_fasta, raw_info, raw_pubs],
    target=['$out/sync.fasta',
            '$out/sync.csv',
            '$out/sync_pubs.csv'],
    action=['sync_records.py --accessions $have_records $SOURCES $TARGETS',
            # overwrite the "raw" files with the sync files
            Copy('${SOURCES[0]}', '${TARGETS[0]}'),
            Copy('${SOURCES[1]}', '${TARGETS[1]}'),
            Copy('${SOURCES[2]}', '${TARGETS[2]}')
            ])

"""
Check sequence orientation and 16s region

TODO: save state of outputs to avoid extra work on records already processed
TODO: analyze no_16s matches, we want to make sure
      we're not losing valid sequences.
"""
orientated, orientated_info, _, _, _ = base_env.Command(
    target=['$out/orientated.fasta',
            '$out/orientated.csv',
            '$out/alignments.csv',
            '$out/notmatched.fasta',
            '$out/notmatched.csv'],
    source=[sync_fasta, '$training_set', sync_info],
    action=('deenurp orientate_sequences '
            '--threads $nproc '
            '--id 0.70 '
            '--log $out/vsearch.log '
            '--seq_info ${SOURCES[2]} '
            '--out ${TARGETS[0]} '
            '--out_seq_info ${TARGETS[1]} '
            '--out_csv ${TARGETS[2]} '
            '--out_notmatched ${TARGETS[3]} '
            '--out_notmatched_info ${TARGETS[4]} '
            '${SOURCES[:2]}'))

# move to version specific refsets
out = os.path.join('$out', str(version))
env = base_env.Clone(out=out)

seqs, pubs, _ = env.Command(
    target=['$out/seqs.fasta', '$out/pubs.csv', '$out/new_accessions.txt'],
    source=[orientated, sync_pubs, accessions],
    action=[Copy('${TARGETS[0]}',  '${SOURCES[0]}'),
            Copy('${TARGETS[1]}',  '${SOURCES[1]}'),
            Copy('${TARGETS[2]}',  '${SOURCES[2]}')])

"""
taxonomy md5 incase we need to update

TODO: move this process into its own pipeline
"""
tax_md5 = env.Command(
    target=['$out/taxdmp.zip.md5'],
    source=None,
    action=('wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip.md5 '
            '--output-document $TARGET'))
# base_env.AlwaysBuild(tax_md5)

"""
download and parse the full ncbi taxonomy
"""
tax_db, taxdmp = env.Command(
    target=['$out/ncbi_taxonomy.db', '$out/taxdmp.zip'],
    source=tax_md5,
    action='taxit new_database --download-dir $out --database-file $TARGET',
    use_cluster=False)

"""
update seq_info with latest tax_ids and add taxid_classified column
"""
seq_info, _ = env.Command(
    target=['$out/seq_info.csv', '$out/unknown_taxons.csv'],
    source=[orientated_info, tax_db],
    action=['taxit update_taxids '
            '--name-column organism '
            '--taxid-classified '
            '--append-lineage species '
            '--unknowns ${TARGETS[1]} '
            '--out ${TARGETS[0]} '
            '$SOURCES'])

"""
Make general taxtable
"""
tax = env.Command(
    target='$out/taxonomy.csv',
    source=[tax_db, seq_info],
    action=('taxit taxtable '
            '--full '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
tax_counts = env.Command(
    target='$out/tax_counts.csv',
    source=[tax, seq_info],
    action=('taxit count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Drop sequences < 1200 bp and with more than 1% ambig and
partition into named and unnamed data sets.
"""
named_fasta, named_seqinfo, unnamed_fasta, unnamed_seqinfo = env.Command(
    target=['$out/named/seqs.fasta', '$out/named/seq_info.csv',
            '$out/unnamed/seqs.fasta', '$out/unnamed/seq_info.csv'],
    source=[orientated, seq_info],
    action=('deenurp partition_refs '
            # filtering
            '--min-length 1200 '
            '--prop-ambig-cutoff 0.01 '
            '--named-seqs ${TARGETS[0]} --named-info ${TARGETS[1]} '
            '--unnamed-seqs ${TARGETS[2]} --unnamed-info ${TARGETS[3]} '
            '$SOURCES')
)

"""
Make named taxtable
"""
named_tax = env.Command(
    target='$out/named/taxonomy.csv',
    source=[tax_db, named_seqinfo],
    action=('taxit taxtable '
            '--full '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
named_tax_counts = env.Command(
    target='$out/named/tax_counts.csv',
    source=[named_tax, named_seqinfo],
    action=('taxit count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Make unnamed taxtable
"""
unnamed_tax = env.Command(
    target='$out/unnamed/taxonomy.csv',
    source=[tax_db, unnamed_seqinfo],
    action=('taxit taxtable '
            '--full '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
unnamed_tax_counts = env.Command(
    target='$out/unnamed/tax_counts.csv',
    source=[unnamed_tax, unnamed_seqinfo],
    action=('taxit count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Create blast database for 1200bp
"""
named1200_blast = blast_db(env, named_fasta, '$out/named/blast')
unnamed1200_blast = blast_db(env, unnamed_fasta, '$out/unnamed/blast')
bp1200_blast = combine_blast_dbs(
    env,
    db_files=Flatten(named1200_blast + unnamed1200_blast),
    output_base='$out/blast/1200',
    title="Combine named and unnamed ncbi")

"""
Partition type strains
"""
types_fasta, types_seqinfo = env.Command(
    target=['$out/types/seqs.fasta', '$out/types/seq_info.csv'],
    source=[named_fasta, named_seqinfo],
    action=('deenurp partition_refs '
            '--type-seqs ${TARGETS[0]} '
            '--type-info ${TARGETS[1]} '
            '$SOURCES'))

"""
Make type strain taxtable
"""
types_tax = env.Command(
    target='$out/types/taxonomy.csv',
    source=[tax_db, types_seqinfo],
    action=('taxit taxtable '
            '--full '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Count reference sequences that can be used for classification filtering
"""
unnamed_ref_counts = env.Command(
    target='$out/types/tax_counts.csv',
    source=[types_tax, types_seqinfo],
    action=('taxit count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Make type strain Blast database
"""
types_blast = blast_db(env, types_fasta, '$out/types/blast')

# Taxonomic filtering

# add environment variables for additional output types
v = Variables()
for dn in 'filtered', 'add_otus', 'expand_names':
    d = os.path.join('$out', 'tax_filter', dn)
    v.Add(
        PathVariable(dn, 'output directory', d, PathVariable.PathIsDirCreate))
    env[dn] = d

"""
Deduplicate sequences by species rank

TODO: Either include that full taxonomy as an argument or engineer a
way to append the 'species' column to the seq_info.csv
"""
dedup_fasta, dedup_info = env.Command(
    target=['$filtered/dedup.fasta',
            '$filtered/dedup.csv'],
    source=[named_fasta, named_seqinfo],
    action=('deenurp deduplicate_sequences '
            '--group-by species '
            '--prefer-columns is_type '
            '$SOURCES $TARGETS'))

# Filter sequences. Use --threads if you need to to limit the number
# of processes - otherwise deenurp will use all of them!
filtered_fasta, filtered_seqinfo, filtered_details = env.Command(
    source=[seq_info, dedup_fasta, dedup_info, named_tax],
    target=['$filtered/seqs.fasta',
            '$filtered/seq_info.csv',
            '$filtered/details.csv'],
    action=('rm -f ${SOURCES[0]}.ssi && '
            'deenurp filter_outliers '
            '--filter-rank species '
            '--strategy cluster '
            '--distance-percentile 90 '
            '--min-distance 0.01 '
            '--max-distance 0.05 '
            '--min-seqs-for-filtering 5 '
            '--threads $nproc '
            '--filtered-seqinfo ${TARGETS[1]} '
            '--detailed-seqinfo ${TARGETS[2]} '
            '--previous-details ${TARGETS[2]} '
            '${SOURCES[1:]} ${TARGETS[0]}'),
    slurm_queue='full')
env.Precious([filtered_fasta, filtered_seqinfo, filtered_details])

"""
Make filtered taxtable
"""
filtered_tax = env.Command(
    target='$filtered/taxonomy.csv',
    source=[tax_db, filtered_seqinfo],
    action=('taxit taxtable '
            '--full '
            '--seq-info ${SOURCES[1]} '
            '--out-file $TARGET '
            '${SOURCES[0]}'))

filtered_blast = blast_db(env, filtered_fasta, '$filtered/blast')

"""
Count reference sequences that can be used for classification filtering
"""
filtered_ref_counts = env.Command(
    target='$filtered/tax_counts.csv',
    source=[filtered_tax, filtered_seqinfo],
    action=('taxit count_taxids '
            '--seq-info ${SOURCES[1]} '
            '--out $TARGET '
            '${SOURCES[0]}'))

"""
Append contributers
"""
contributors = env.Command(
    source='.git/logs/HEAD',
    target='contributors.txt',
    action=('git log --all --format="%cN <%cE>" | sort | uniq > $TARGET'))
